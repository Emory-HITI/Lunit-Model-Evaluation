{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988cb892-2bff-49e1-be59-b21ffeed60ee",
   "metadata": {},
   "source": [
    "# Lunit Study Screening Label Assignment v6.2\n",
    "### Beatrice Brown-Mulry\n",
    "### 06/16/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b8142c-8a26-4b88-9740-bcee5353af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import embed_toolkit  # version 0.2.*\n",
    "from typing import Optional, Union\n",
    "import bisect\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "tqdm.pandas()  # initialize tqdm wrapper for pandas.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeba4ce-6345-4550-93be-319d0b4d90d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGVIEW_PATH = \"/mnt/NAS3/mammo/lunit/data/standalone_studies_clairity_anon.csv\"\n",
    "SCORE_PATH = \"../../data/dbt_cohort_scores_v2.csv\"\n",
    "ADDENDUMS_PATH = \"/mnt/NAS3/mammo/tables/embedv2_extract_current_anon/Addendums_anon.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc92a7-58c6-4bef-bd62-c3aa2915c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init embed params object\n",
    "embed_params = embed_toolkit.EMBEDParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ee738",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 0. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7cde75",
   "metadata": {},
   "source": [
    "## 0.1 Prepare Magview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e97e6-4556-445e-937e-b1429d65c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "mag_df = pd.read_csv(MAGVIEW_PATH)\n",
    "mag_df = mag_df.dropna(subset=\"desc\")\n",
    "\n",
    "# ensure key columns have the correct data types\n",
    "mag_df[\"empi_anon\"] = pd.to_numeric(mag_df[\"empi_anon\"])\n",
    "mag_df[\"acc_anon\"] = pd.to_numeric(mag_df[\"acc_anon\"])\n",
    "mag_df[\"study_date_anon\"] = pd.to_datetime(mag_df[\"study_date_anon\"])\n",
    "\n",
    "# drop existing finding type cols so we can re-derive them\n",
    "mag_drop_cols = [\"arch_distortion\", \"asymmetry\", \"calc\", \"mass\"]\n",
    "mag_df = mag_df.drop(columns=mag_drop_cols)\n",
    "\n",
    "# exclude invalid accessions (coded as 999)\n",
    "mag_df = mag_df[mag_df.acc_anon != 999]\n",
    "\n",
    "# get subset of magview corresponding to screen exams\n",
    "mag_scr = mag_df.loc[mag_df.desc.str.contains(\"scr\", case=False)]\n",
    "\n",
    "# summarize dataframe contents\n",
    "mag_scr.embed.summarize(\"Screen Magview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9abf8-5d47-426b-894e-85329cbbb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of magview corresponding to diagnostic exams\n",
    "mag_diag = mag_df.loc[\n",
    "    mag_df.desc.str.contains(\"diag\", case=False) | mag_df.desc.str.contains(\"US\")\n",
    "]\n",
    "\n",
    "# summarize dataframe contents\n",
    "mag_diag.embed.summarize(\"Diagnostic Magview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd312d",
   "metadata": {},
   "source": [
    "## 0.2 Correct Contralaterals\n",
    "We need to run the contralateral correction function on our Magview dataframe to create entries for any negative contralateral findings that are implied on bilateral exams with single-sided unilateral findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a09638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply contralateral correction\n",
    "mag_scr_contra = embed_toolkit.correct_contralaterals(mag_scr)\n",
    "\n",
    "# correct column dtypes\n",
    "mag_scr_contra[\"study_date_anon\"] = pd.to_datetime(mag_scr_contra[\"study_date_anon\"])\n",
    "mag_scr_contra[\"acc_anon\"] = pd.to_numeric(mag_scr_contra[\"acc_anon\"])\n",
    "mag_scr_contra[\"empi_anon\"] = pd.to_numeric(mag_scr_contra[\"empi_anon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22411ca-a63f-43e4-abc9-c2e15414b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert any 'B' findings to separate 'L'/'R' findings\n",
    "new_rows = []\n",
    "\n",
    "for _, row in mag_scr_contra.iterrows():\n",
    "    if row[\"side\"] == \"B\":\n",
    "        row_L = row.copy()\n",
    "        row_L[\"side\"] = \"L\"\n",
    "        row_R = row.copy()\n",
    "        row_R[\"side\"] = \"R\"\n",
    "        new_rows.extend([row_L, row_R])\n",
    "    else:\n",
    "        new_rows.append(row)\n",
    "\n",
    "# create a new DataFrame from the expanded rows\n",
    "mag_scr_contra = pd.DataFrame(new_rows).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e3cd4-5b78-4021-858a-c1cd09291cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new acc_lat identifier to track labels per exam side\n",
    "mag_scr_contra[\"acc_lat\"] = (\n",
    "    mag_scr_contra[\"acc_anon\"].astype(str) + \"_\" + mag_scr_contra[\"side\"].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30d3a5-1c84-4f77-a7b0-ca5ea36b024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply contralateral correction\n",
    "mag_diag_contra = embed_toolkit.correct_contralaterals(mag_diag)\n",
    "\n",
    "# correct column dtypes\n",
    "mag_diag_contra[\"study_date_anon\"] = pd.to_datetime(mag_diag_contra[\"study_date_anon\"])\n",
    "mag_diag_contra[\"acc_anon\"] = pd.to_numeric(mag_diag_contra[\"acc_anon\"])\n",
    "mag_diag_contra[\"empi_anon\"] = pd.to_numeric(mag_diag_contra[\"empi_anon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba783c8-01e6-435c-ac7b-8f0f9fa19a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert any 'B' findings to separate 'L'/'R' findings\n",
    "new_rows = []\n",
    "\n",
    "for _, row in mag_diag_contra.iterrows():\n",
    "    if row[\"side\"] == \"B\":\n",
    "        row_L = row.copy()\n",
    "        row_L[\"side\"] = \"L\"\n",
    "        row_R = row.copy()\n",
    "        row_R[\"side\"] = \"R\"\n",
    "        new_rows.extend([row_L, row_R])\n",
    "    else:\n",
    "        new_rows.append(row)\n",
    "\n",
    "# create a new DataFrame from the expanded rows\n",
    "mag_diag_contra = pd.DataFrame(new_rows).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf48031-f128-49e6-a664-11b43dd3c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new acc_lat identifier to track labels per exam side\n",
    "mag_diag_contra[\"acc_lat\"] = (\n",
    "    mag_diag_contra[\"acc_anon\"].astype(str) + \"_\" + mag_diag_contra[\"side\"].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ac2c0b",
   "metadata": {},
   "source": [
    "## 0.3 Handle Addended Exam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8815807-425f-453e-addc-796d6dd15ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding matching algorithm -------------------------------------------------------------\n",
    "class MatchCandidate:\n",
    "    def __init__(self, base_row: pd.Series, target_row: pd.Series):\n",
    "        # get the list of columns shared by both\n",
    "        self.cols: list[str] = list(\n",
    "            set(base_row.keys()).intersection(set(target_row.keys()))\n",
    "        )\n",
    "\n",
    "        # extract the relevant columns from base and target\n",
    "        self.base: dict[str, any] = self._clean(base_row[self.cols])\n",
    "        self.target: dict[str, any] = self._clean(target_row[self.cols])\n",
    "\n",
    "        # reserve matching dict for later\n",
    "        self.matches: dict[str, bool] = dict()\n",
    "\n",
    "    def evaluate(self) -> float:\n",
    "        self.matches: dict[str, bool] = dict()\n",
    "\n",
    "        for k in self.cols:\n",
    "            base_v: any = self.base[k]\n",
    "            target_v: any = self.target[k]\n",
    "            is_match: bool = base_v == target_v\n",
    "\n",
    "            # evaluate matches\n",
    "            self.matches[k] = is_match\n",
    "\n",
    "        n_matches: int = sum(list(self.matches.values()))\n",
    "\n",
    "        return n_matches / len(self.matches)\n",
    "\n",
    "    def get_mismatches(self) -> dict[str, dict[str, any]]:\n",
    "        mismatches: dict[str, dict[str, any]] = dict()\n",
    "\n",
    "        if len(self.matches) == 0:\n",
    "            _ = self.evaluate()\n",
    "\n",
    "        mismatch_keys: list[str] = [k for k, v in self.matches.items() if v == False]\n",
    "        for k in mismatch_keys:\n",
    "            mismatches[k] = {\"base\": self.base[k], \"target\": self.target[k]}\n",
    "\n",
    "        return mismatches\n",
    "\n",
    "    def _clean(self, data: pd.Series) -> dict[str, any]:\n",
    "        # init output dict\n",
    "        clean_data: dict[str, any] = dict()\n",
    "\n",
    "        # iterate over raw keys/values\n",
    "        for k, v in data.items():\n",
    "            str_k: str = str(k)\n",
    "            # handle nans\n",
    "            if pd.isnull(v):\n",
    "                clean_data[str_k] = \"nan\"\n",
    "                continue\n",
    "\n",
    "            # otherwise, cast to string and handle\n",
    "            # (also catches values that were already strings)\n",
    "            else:\n",
    "                str_v: str = str(v).strip()\n",
    "\n",
    "                # treat empty strings like nans\n",
    "                if len(str_v) == 0:\n",
    "                    clean_data[str_k] = \"nan\"\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    float_v: Union[float, bool] = is_str_number(str_v)\n",
    "                    # print(f\"{str_k}: {float_v}\")\n",
    "                    if isinstance(float_v, float):\n",
    "                        clean_data[str_k] = float_v\n",
    "                        continue\n",
    "\n",
    "                    else:\n",
    "                        # otherwise write the string to the output\n",
    "                        clean_data[str_k] = str_v\n",
    "                        continue\n",
    "\n",
    "        return clean_data\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# matching scores data structure -----------------------------------------------\n",
    "@dataclass(order=True)\n",
    "class ScoredItem:\n",
    "    # negative score for max-heap like sorting (highest score first)\n",
    "    sort_index: float = field(init=False, repr=False)\n",
    "    score: float\n",
    "    source_num: int\n",
    "    add_idx: int\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # sort by negative score so highest score comes first\n",
    "        self.sort_index = -self.score\n",
    "\n",
    "\n",
    "class ScoredIndexStore:\n",
    "    def __init__(self):\n",
    "        self.items: list[ScoredItem] = []\n",
    "        self.by_source_num: dict[int, list[ScoredItem]] = {}\n",
    "        self.by_add_idx: dict[int, list[ScoredItem]] = {}\n",
    "\n",
    "    def add(self, source_num: int, add_idx: int, score: float):\n",
    "        item = ScoredItem(score=score, source_num=source_num, add_idx=add_idx)\n",
    "        bisect.insort(self.items, item)\n",
    "        self.by_source_num.setdefault(source_num, []).append(item)\n",
    "        self.by_add_idx.setdefault(add_idx, []).append(item)\n",
    "\n",
    "    def filter_by_source_num(self, source_num: int) -> list[ScoredItem]:\n",
    "        return self.by_source_num.get(source_num, [])\n",
    "\n",
    "    def filter_by_add_idx(self, add_idx: int) -> list[ScoredItem]:\n",
    "        return self.by_add_idx.get(add_idx, [])\n",
    "\n",
    "    def remove_by_source_num(self, source_num: int):\n",
    "        # get items associated with the source num\n",
    "        to_remove = self.by_source_num.pop(source_num, [])\n",
    "        for item in to_remove:\n",
    "            # remove each item from the main item list\n",
    "            self.items.remove(item)\n",
    "\n",
    "            # remove related items from the by_add_idx dict\n",
    "            self.by_add_idx[item.add_idx].remove(item)\n",
    "            if not self.by_add_idx[item.add_idx]:\n",
    "                del self.by_add_idx[item.add_idx]\n",
    "\n",
    "    def remove_by_add_idx(self, add_idx: int):\n",
    "        # get items associated with the addendum num\n",
    "        to_remove = self.by_add_idx.pop(add_idx, [])\n",
    "        for item in to_remove:\n",
    "            # remove each item from the main item list\n",
    "            self.items.remove(item)\n",
    "\n",
    "            # remove related items from the by_source_num dict\n",
    "            self.by_source_num[item.source_num].remove(item)\n",
    "            if not self.by_source_num[item.source_num]:\n",
    "                del self.by_source_num[item.source_num]\n",
    "\n",
    "    def get_sorted_items(self) -> list[ScoredItem]:\n",
    "        return list(self.items)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# mapping data structures ------------------------------------------------------\n",
    "@dataclass\n",
    "class MappingPair:\n",
    "    source: Optional[int]  # source row index\n",
    "    addendum: Optional[int]  # addendum row index\n",
    "    append: bool = False  # can also be inferred by a None index\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AddendumMapping:\n",
    "    acc_lat: str  # exam access\n",
    "    mappings: list[MappingPair] = field(default_factory=list)\n",
    "\n",
    "    def append(self, item: MappingPair):\n",
    "        self.mappings.append(item)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# match breast-level slices ----------------------------------------------------\n",
    "def match_slices(\n",
    "    acc_lat: str, mag_slice: pd.DataFrame, add_slice: pd.DataFrame\n",
    ") -> AddendumMapping:\n",
    "    # init struct to track output mappings\n",
    "    mappings: AddendumMapping = AddendumMapping(acc_lat)\n",
    "\n",
    "    # get lists to track our indexes\n",
    "\n",
    "    mag_numfind_idxs: dict[int, list[int]] = dict()\n",
    "    for numfind, group in mag_slice.groupby(\"numfind\"):  # type: ignore\n",
    "        idxs: list[int] = group.index.tolist()\n",
    "        mag_numfind_idxs[numfind] = idxs  # type: ignore\n",
    "\n",
    "    # mag_numfinds: list[int] = mag_slice.numfind.astype(int).unique().tolist()  # type: ignore\n",
    "    add_indexes: list[int] = add_slice.index.tolist()\n",
    "\n",
    "    # handle special cases that don't require matching\n",
    "    # 0:n\n",
    "    if len(mag_numfind_idxs) == 0:\n",
    "        for add_idx in add_indexes:\n",
    "            mappings.append(MappingPair(source=None, addendum=add_idx, append=True))\n",
    "        return mappings\n",
    "\n",
    "    # 1:1\n",
    "    elif (len(mag_numfind_idxs) == 1) & (len(add_indexes) == 1):\n",
    "        # get the single numfind and retrieve its associated idxs\n",
    "        numfind: int = list(mag_numfind_idxs.keys())[0]\n",
    "        idxs: list[int] = mag_numfind_idxs[numfind]\n",
    "\n",
    "        # add them to the mappings object\n",
    "        for source_idx in idxs:\n",
    "            mappings.append(MappingPair(source=source_idx, addendum=add_indexes[0]))\n",
    "\n",
    "        return mappings\n",
    "\n",
    "    # handle cases that require matching\n",
    "    else:\n",
    "        # init data struct to track row similarities\n",
    "        store = ScoredIndexStore()\n",
    "\n",
    "        for i, mag_row in mag_slice.iterrows():\n",
    "            for j, add_row in add_slice.iterrows():\n",
    "                # init candidate object and get match score\n",
    "                candidate = MatchCandidate(mag_row, add_row)\n",
    "                match_score = candidate.evaluate()\n",
    "\n",
    "                # register the evaluation in the store\n",
    "                store.add(mag_row.numfind, j, match_score)  # type: ignore\n",
    "\n",
    "        # get the top match score from the store\n",
    "        while True:\n",
    "            # if the mag numfind list is empty\n",
    "            # append all remaining addendums\n",
    "            if len(mag_numfind_idxs) == 0:\n",
    "                for add_idx in add_indexes:\n",
    "                    mappings.append(\n",
    "                        MappingPair(source=None, addendum=add_idx, append=True)\n",
    "                    )\n",
    "                break\n",
    "\n",
    "            # else if the add index list is empty\n",
    "            # append all remaining magview rows\n",
    "            elif len(add_indexes) == 0:\n",
    "                for numfind, idxs in mag_numfind_idxs.items():\n",
    "                    for source_idx in idxs:\n",
    "                        mappings.append(\n",
    "                            MappingPair(\n",
    "                                source=source_idx,\n",
    "                                addendum=None,\n",
    "                                append=True,\n",
    "                            )\n",
    "                        )\n",
    "                break\n",
    "\n",
    "            # handle 1:1 matches so we can skip unneeded computation\n",
    "            elif (len(mag_numfind_idxs) == 1) & (len(add_indexes) == 1):\n",
    "                # get the single numfind and retrieve its associated idxs\n",
    "                numfind: int = list(mag_numfind_idxs.keys())[0]\n",
    "                idxs: list[int] = mag_numfind_idxs[numfind]\n",
    "\n",
    "                # add them to the mappings object\n",
    "                for source_idx in idxs:\n",
    "                    mappings.append(\n",
    "                        MappingPair(source=source_idx, addendum=add_indexes[0])\n",
    "                    )\n",
    "\n",
    "                break\n",
    "\n",
    "            # get the highest match score\n",
    "            top_match = store.get_sorted_items()[0]\n",
    "\n",
    "            # extract match indices\n",
    "            match_mag_num = top_match.source_num\n",
    "            match_add_idx = top_match.add_idx\n",
    "\n",
    "            # get the indices associated with the numfind\n",
    "            idxs: list[int] = mag_numfind_idxs.pop(match_mag_num)\n",
    "\n",
    "            for source_idx in idxs:\n",
    "                # append match to mapping obj\n",
    "                mappings.append(\n",
    "                    MappingPair(\n",
    "                        source=source_idx,\n",
    "                        addendum=match_add_idx,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # remove all entries matching the numfind from the store\n",
    "            store.remove_by_source_num(match_mag_num)\n",
    "\n",
    "            # remove matched addendum rows by index\n",
    "            add_indexes.remove(match_add_idx)\n",
    "            store.remove_by_add_idx(match_add_idx)\n",
    "\n",
    "        return mappings\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# exam-level addendum matching -------------------------------------------------\n",
    "def resolve_addendums(\n",
    "    mag_exam_slice: pd.DataFrame,\n",
    "    add_exam_slice: pd.DataFrame,\n",
    "    exam_cols: list[str],\n",
    "    add_cols: list[str],\n",
    ") -> pd.DataFrame:\n",
    "    out_data: list[pd.Series] = []\n",
    "\n",
    "    mag_exam_data = mag_exam_slice[exam_cols].iloc[0]  # .set_index(pd.Index([0]))\n",
    "\n",
    "    mag_slice_cols: list[str] = list(\n",
    "        set(mag_exam_slice.columns).difference(set(add_cols))\n",
    "    )\n",
    "    acc_lat_list: list[str] = list(\n",
    "        set(mag_exam_slice.acc_lat).union(set(add_exam_slice.acc_lat))\n",
    "    )\n",
    "\n",
    "    for acc_lat in acc_lat_list:\n",
    "        add_slice = add_exam_slice[add_exam_slice.acc_lat == acc_lat]\n",
    "        mag_slice = mag_exam_slice[mag_exam_slice.acc_lat == acc_lat]\n",
    "\n",
    "        # match mag/add slices and get the output mapping\n",
    "        mappings: AddendumMapping = match_slices(acc_lat, mag_slice, add_slice)\n",
    "\n",
    "        # iterate over mappings\n",
    "        for map_obj in mappings.mappings:\n",
    "            mag_idx: Optional[int] = map_obj.source\n",
    "            add_idx: Optional[int] = map_obj.addendum\n",
    "\n",
    "            if (map_obj.append == True) & (mag_idx is None):\n",
    "                # append addendum only\n",
    "                row_data: pd.Series = pd.concat(\n",
    "                    [mag_exam_data, add_slice.loc[add_idx, add_cols]]  # type: ignore\n",
    "                )\n",
    "                out_data.append(row_data)\n",
    "\n",
    "            elif (map_obj.append == True) & (add_idx is None):\n",
    "                # append source only\n",
    "                out_data.append(mag_slice.loc[mag_idx])  # type: ignore\n",
    "\n",
    "            else:\n",
    "                # merge matched indexes\n",
    "                row_data: pd.Series = pd.concat(\n",
    "                    [\n",
    "                        mag_slice.loc[mag_idx, mag_slice_cols],  # type: ignore\n",
    "                        add_slice.loc[add_idx, add_cols],  # type: ignore\n",
    "                    ]\n",
    "                )\n",
    "                row_data[\"numfind\"] = mag_slice.loc[mag_idx, \"numfind\"]  # type: ignore\n",
    "                out_data.append(row_data)\n",
    "\n",
    "    return pd.DataFrame(out_data, columns=mag_exam_slice.columns)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# utility funcs ----------------------------------------------------------------\n",
    "def is_str_number(v: str) -> Union[float, False]:\n",
    "    try:\n",
    "        float_v = float(v)\n",
    "        return float_v\n",
    "\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af4f10-d1f1-4818-a24d-fb1387d8961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_df = pd.read_csv(ADDENDUMS_PATH)\n",
    "\n",
    "# correct variable dtypes and rename columns\n",
    "add_rename_dict = {\n",
    "    \"accession_anon\": \"acc_anon\",\n",
    "    \"EMPI_anon\": \"empi_anon\",\n",
    "    \"studydate_ANON\": \"study_date_anon\",\n",
    "    \"dt_final_ANON\": \"dt_final_anon\",\n",
    "    \"dt_rel_ANON\": \"dt_rel_anon\",\n",
    "    \"linkedaccession_anon\": \"linked_acc_anon\",\n",
    "    \"procdate_ANON\": \"procdate_anon\",\n",
    "    \"save_date_ANON\": \"save_date_anon\",\n",
    "    \"pdate_ANON\": \"pdate_anon\",\n",
    "    \"patientage_anon\": \"age_at_study\",\n",
    "    \"depth.1\": \"bdepth\",\n",
    "    \"distance.1\": \"bdistance\",\n",
    "    \"side.1\": \"bside\",\n",
    "}\n",
    "add_df.rename(columns=add_rename_dict, inplace=True)\n",
    "add_df[\"acc_anon\"] = pd.to_numeric(add_df[\"acc_anon\"])\n",
    "\n",
    "# drop unnecessary columns\n",
    "add_drop_cols = [\n",
    "    \"comment\",\n",
    "    \"path_dr\",\n",
    "    \"init.1\",\n",
    "    \"mdelayed.1\",\n",
    "    \"save_date_anon\",\n",
    "    \"secondaryADDENDUM\",\n",
    "    \"surgeon\",\n",
    "    \"numfind\",\n",
    "]\n",
    "add_df = add_df.drop(columns=add_drop_cols)\n",
    "\n",
    "# summarize dataframe contents\n",
    "add_df.embed.summarize(\"Addended Exams\")\n",
    "print(len(add_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cca55-f015-471c-b7ab-c413bc52beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in cases where an exam had multiple addendums, filter them to only consider the final addendum results\n",
    "max_anum_dict = (\n",
    "    add_df.groupby(\"acc_anon\").apply(lambda group: group.anum.max()).to_dict()\n",
    ")\n",
    "\n",
    "add_df[\"max_anum\"] = add_df[\"acc_anon\"].map(max_anum_dict)\n",
    "add_df[\"max_anum\"].value_counts()\n",
    "\n",
    "add_df = add_df[add_df[\"anum\"] == add_df[\"max_anum\"]]\n",
    "add_df.embed.summarize(\"Filtered Addended Exams\")\n",
    "print(len(add_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae64c90d-3872-4caa-a993-9421b2ef6cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert any 'B' findings to separate 'L'/'R' findings\n",
    "new_rows = []\n",
    "\n",
    "for _, row in add_df.iterrows():\n",
    "    if (row[\"side\"] == \"B\") or (row[\"side\"].strip() == \"\"):\n",
    "        row_L = row.copy()\n",
    "        row_L[\"side\"] = \"L\"\n",
    "        row_R = row.copy()\n",
    "        row_R[\"side\"] = \"R\"\n",
    "        new_rows.extend([row_L, row_R])\n",
    "    else:\n",
    "        new_rows.append(row)\n",
    "\n",
    "# create a new DataFrame from the expanded rows\n",
    "add_df = pd.DataFrame(new_rows).reset_index(drop=True)\n",
    "\n",
    "# create a new acc_lat identifier to track labels per exam side\n",
    "add_df[\"acc_lat\"] = add_df[\"acc_anon\"].astype(str) + \"_\" + add_df[\"side\"].astype(str)\n",
    "\n",
    "add_df.side.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84756de1-6c65-42f4-b59b-057cd1dbc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the addendum subsets for scr/diag exams\n",
    "scr_add_df = add_df[add_df.desc.str.contains(\"scr\", case=False)]\n",
    "diag_add_df = add_df[add_df.desc.str.contains(\"diag\", case=False)]\n",
    "\n",
    "# summarize dataframe contents\n",
    "scr_add_df.embed.summarize(\"Addended Screen Exams\")\n",
    "diag_add_df.embed.summarize(\"Addended Diagnostic Exams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e2d92d-c79c-46a5-81fa-376fb681b625",
   "metadata": {},
   "source": [
    "### 0.3.1 Handle Screen Addendums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51110b7-47a6-403b-9c57-04325f0a83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_accs: list[int] = scr_add_df.acc_anon.unique().tolist()  # type: ignore\n",
    "scr_acc_lats: list[int] = scr_add_df.acc_lat.unique().tolist()  # type: ignore\n",
    "\n",
    "print(f\"{len(scr_accs):,} addended exams\")\n",
    "print(f\"{len(scr_acc_lats):,} addended exams-sides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fda85c-097e-4785-9153-b790d871dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get exam level source columns to extract when merging\n",
    "exam_cols = (\n",
    "    embed_params.level_columns[\"patient\"]\n",
    "    + [\n",
    "        \"height\",\n",
    "        \"weight\",\n",
    "        \"livebirths\",\n",
    "        \"menopauseage\",\n",
    "        \"pregnancyage\",\n",
    "        \"menarcheage\",\n",
    "        \"ASHKENAZI\",\n",
    "    ]\n",
    "    + embed_params.level_columns[\"exam\"]\n",
    "    + embed_params.level_columns[\"procedure\"]\n",
    ")\n",
    "exam_cols = [c for c in exam_cols if c in list(mag_scr_contra.columns)]\n",
    "\n",
    "# get addendum columns to extract when merging\n",
    "add_cols = [\n",
    "    c for c in add_df.columns if (c in mag_scr_contra.columns) and (c not in exam_cols)\n",
    "]\n",
    "\n",
    "resolved_data: list[pd.DataFrame] = []\n",
    "scr_acc_list = scr_add_df.acc_anon.unique().tolist()\n",
    "\n",
    "for acc in tqdm(scr_acc_list):\n",
    "    # get exam subsets\n",
    "    mag_exam_slice = mag_scr_contra[mag_scr_contra.acc_anon == acc]\n",
    "    add_exam_slice = scr_add_df[scr_add_df.acc_anon == acc]\n",
    "\n",
    "    # skip resolution of addendums if the acc_anon doesn't exist in the magview sample at all\n",
    "    if len(mag_exam_slice) == 0:\n",
    "        continue\n",
    "\n",
    "    # append resolved data to the output list\n",
    "    resolved_data.append(\n",
    "        resolve_addendums(mag_exam_slice, add_exam_slice, exam_cols, add_cols)\n",
    "    )\n",
    "\n",
    "scr_resolved_df = pd.concat(resolved_data, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e9492-41a0-4b5e-be24-55cd6b925f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_resolved_df.calcnumber = scr_resolved_df.calcnumber.fillna(0)\n",
    "scr_resolved_df.calcnumber.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5996bcc1-2785-44d2-8a40-6dc7b243eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "addended_accs = scr_resolved_df.acc_anon.unique().tolist()\n",
    "\n",
    "mag_scr_add = pd.concat(\n",
    "    [mag_scr_contra[~mag_scr_contra.acc_anon.isin(addended_accs)], scr_resolved_df],\n",
    "    axis=0,\n",
    ")\n",
    "mag_scr_add = mag_scr_add.sort_values([\"acc_anon\", \"side\"]).reset_index(drop=True)\n",
    "\n",
    "print(len(mag_scr_add))\n",
    "mag_scr_add.embed.summarize(\"Magview Screen Post-Addendum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f7b13-9e23-43dc-8a7f-4be9350b2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mag_scr_add.asses.value_counts(dropna=False))\n",
    "\n",
    "# exclude screen findings with a missing BI-RADS assessment\n",
    "mag_scr_add = mag_scr_add[~pd.isna(mag_scr_add.asses)]\n",
    "\n",
    "display(mag_scr_add.asses.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e58bd5-9fff-43d2-84c9-57fda712632b",
   "metadata": {},
   "source": [
    "### 0.3.2 Handle Diagnostic Addendums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01ddc70-22a2-4663-8349-0302ed2ce5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_accs: list[int] = diag_add_df.acc_anon.unique().tolist()  # type: ignore\n",
    "dx_acc_lats: list[int] = diag_add_df.acc_lat.unique().tolist()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0527e22-48f6-4fe2-890e-8ea8e25b9729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get exam level source columns to extract when merging\n",
    "exam_cols = (\n",
    "    embed_params.level_columns[\"patient\"]\n",
    "    + [\n",
    "        \"height\",\n",
    "        \"weight\",\n",
    "        \"livebirths\",\n",
    "        \"menopauseage\",\n",
    "        \"pregnancyage\",\n",
    "        \"menarcheage\",\n",
    "        \"ASHKENAZI\",\n",
    "    ]\n",
    "    + embed_params.level_columns[\"exam\"]\n",
    "    + embed_params.level_columns[\"procedure\"]\n",
    ")\n",
    "exam_cols = [c for c in exam_cols if c in list(mag_diag_contra.columns)]\n",
    "\n",
    "# get addendum columns to extract when merging\n",
    "add_cols = [\n",
    "    c for c in add_df.columns if (c in mag_diag_contra.columns) and (c not in exam_cols)\n",
    "]\n",
    "\n",
    "resolved_data: list[pd.DataFrame] = []\n",
    "diag_acc_list = diag_add_df.acc_anon.unique().tolist()\n",
    "\n",
    "for acc in tqdm(diag_acc_list):\n",
    "    # get exam subsets\n",
    "    mag_exam_slice = mag_diag_contra[mag_diag_contra.acc_anon == acc]\n",
    "    add_exam_slice = diag_add_df[diag_add_df.acc_anon == acc]\n",
    "\n",
    "    # skip resolution of addendums if the acc_anon doesn't exist in the magview sample at all\n",
    "    if len(mag_exam_slice) == 0:\n",
    "        continue\n",
    "\n",
    "    # append resolved data to the output list\n",
    "    resolved_data.append(\n",
    "        resolve_addendums(mag_exam_slice, add_exam_slice, exam_cols, add_cols)\n",
    "    )\n",
    "\n",
    "diag_resolved_df = pd.concat(resolved_data, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c3ad9-cef0-4540-927e-90dac9559975",
   "metadata": {},
   "outputs": [],
   "source": [
    "addended_accs = diag_resolved_df.acc_anon.unique().tolist()\n",
    "\n",
    "mag_diag_add = pd.concat(\n",
    "    [mag_diag_contra[~mag_diag_contra.acc_anon.isin(addended_accs)], diag_resolved_df],\n",
    "    axis=0,\n",
    ")\n",
    "mag_diag_add = mag_diag_add.sort_values([\"acc_anon\", \"side\"]).reset_index(drop=True)\n",
    "\n",
    "print(len(mag_diag_add))\n",
    "mag_diag_add.embed.summarize(\"Magview Diagnostic Post-Addendum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c577a-3559-4eb9-87d2-cea587970e16",
   "metadata": {},
   "source": [
    "## 0.4 Derive Exam-Type Laterality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exam_laterality(row: pd.Series) -> str | None:\n",
    "    # extract description and lowercase it\n",
    "    finding_desc = row.desc.lower()\n",
    "\n",
    "    if \"bilat\" in finding_desc:\n",
    "        return \"B\"\n",
    "    elif \"left\" in finding_desc:\n",
    "        return \"L\"\n",
    "    elif \"right\" in finding_desc:\n",
    "        return \"R\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# derive exam laterality from their descriptions\n",
    "mag_scr_add[\"exam_laterality\"] = mag_scr_add.progress_apply(get_exam_laterality, axis=1)  # type: ignore\n",
    "mag_diag_add[\"exam_laterality\"] = mag_diag_add.progress_apply(get_exam_laterality, axis=1)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619456c-34a9-47af-a45e-7cb7c036a9f5",
   "metadata": {},
   "source": [
    "## 0.5 Derive Exam-Level BIRADS and Path.\n",
    "Get the most severe BIRADS and path_severity associated with each exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce096b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each screening exam, take the most severe birads as the representative\n",
    "def get_worst_ps(group):\n",
    "    return group.path_severity.min()\n",
    "\n",
    "\n",
    "def get_worst_br(group):\n",
    "    exam_desc = group.desc.tolist()[0]\n",
    "    if \"screen\" in exam_desc.lower():\n",
    "        br_to_val_dict = {\n",
    "            \"A\": 0,  # 'A' maps to birads 0\n",
    "            \"B\": 1,  # 'B' maps to birads 2\n",
    "            \"N\": 2,  # 'N' maps to birads 1\n",
    "        }\n",
    "    else:\n",
    "        br_to_val_dict = {\n",
    "            \"A\": 6,  # 'A' shouldn't exist here, but it might\n",
    "            \"N\": 5,  # 'N' maps to birads 1\n",
    "            \"B\": 4,  # 'B' maps to birads 2\n",
    "            \"P\": 3,  # 'P' maps to birads 3\n",
    "            \"S\": 2,  # 'S' maps to birads 4\n",
    "            \"M\": 1,  # 'M' maps to birads 5\n",
    "            \"K\": 0,  # 'K' maps to birads 6\n",
    "        }\n",
    "\n",
    "    val_to_br_dict = {v: k for k, v in br_to_val_dict.items()}\n",
    "    worst_br_val = min(group.asses.map(br_to_val_dict).tolist())\n",
    "    return val_to_br_dict.get(worst_br_val, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a355b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_scr_br_dict = (\n",
    "    mag_scr_add.groupby(\"acc_anon\").progress_apply(get_worst_br).to_dict() # type: ignore\n",
    ")\n",
    "worst_diag_br_dict = (\n",
    "    mag_diag_add.groupby(\"acc_anon\").progress_apply(get_worst_br).to_dict() # type: ignore\n",
    ")\n",
    "\n",
    "# map back to magview\n",
    "mag_scr_add[\"exam_birads\"] = \"\"\n",
    "mag_scr_add[\"exam_birads\"] = mag_scr_add[\"acc_anon\"].map(worst_scr_br_dict)\n",
    "\n",
    "mag_diag_add[\"exam_birads\"] = \"\"\n",
    "mag_diag_add[\"exam_birads\"] = mag_diag_add[\"acc_anon\"].map(worst_diag_br_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b3221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the 'get_worst_ps' function to the data (grouped by exam) and output [exam > pathology] mappings as a dict\n",
    "# don't apply it to exam findings with no path severity (since they can't affect results)\n",
    "worst_scr_path_dict = (\n",
    "    mag_scr_add[~pd.isnull(mag_scr_add.path_severity)]\n",
    "    .groupby(\"acc_anon\")\n",
    "    .progress_apply(get_worst_ps)  # type: ignore\n",
    "    .to_dict()\n",
    ")\n",
    "worst_diag_path_dict = (\n",
    "    mag_diag_add[~pd.isnull(mag_diag_add.path_severity)]\n",
    "    .groupby(\"acc_anon\")\n",
    "    .progress_apply(get_worst_ps)  # type: ignore\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# map back to magview\n",
    "mag_scr_add[\"exam_path_severity\"] = np.nan\n",
    "mag_scr_add[\"exam_path_severity\"] = mag_scr_add[\"acc_anon\"].map(worst_scr_path_dict)\n",
    "\n",
    "mag_diag_add[\"exam_path_severity\"] = np.nan\n",
    "mag_diag_add[\"exam_path_severity\"] = mag_diag_add[\"acc_anon\"].map(worst_diag_path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99de819-6e76-4272-84cb-205ddd30303f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "096e91b3-03b5-4b82-9ed2-2eeef819927d",
   "metadata": {},
   "source": [
    "## 0.6 Aggregate Exam Biopsy Sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf2688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_bsides(group):\n",
    "    # applied to exam groups\n",
    "    bside_list = group.bside.unique().tolist()\n",
    "    # return the only bside if we only have 1 (this should never be 0 since NaN is included)\n",
    "    # this should return an IndexError if it ever is 0\n",
    "    if len(bside_list) == 1:\n",
    "        return bside_list[0]\n",
    "\n",
    "    # otherwise aggregate bilateral bsides\n",
    "    elif (\"B\" in bside_list) or ((\"L\" in bside_list) & (\"R\" in bside_list)):\n",
    "        return \"B\"\n",
    "    # handle left bsides with no right or 'B' (other is a NaN)\n",
    "    elif \"L\" in bside_list:\n",
    "        return \"L\"\n",
    "    # handle right bsides with no left or 'B' (other is a NaN)\n",
    "    elif \"R\" in bside_list:\n",
    "        return \"R\"\n",
    "    else:\n",
    "        return \"ERROR\"\n",
    "\n",
    "\n",
    "def get_bside_aggregation_dict(df: pd.DataFrame) -> dict[float, str]:\n",
    "    # we only need to apply this to exam findings with no exam-level pathology registered\n",
    "    path_na_mask: pd.Series[bool] = pd.isna(df.exam_path_severity)\n",
    "\n",
    "    # or exam findings where the finding-level path severity matches the exam-level path severity\n",
    "    path_match_mask: pd.Series[bool] = ~pd.isna(df.exam_path_severity) & (\n",
    "        df.path_severity == df.exam_path_severity\n",
    "    )\n",
    "\n",
    "    # define a list of columns to consider\n",
    "    col_list: list[str] = [\n",
    "        \"acc_anon\",\n",
    "        \"empi_anon\",\n",
    "        \"study_date_anon\",\n",
    "        \"exam_birads\",\n",
    "        \"exam_path_severity\",\n",
    "        \"bside\",\n",
    "    ]\n",
    "\n",
    "    # get the relevant subset of the data, then\n",
    "    df_subset: pd.DataFrame = df.loc[path_na_mask | path_match_mask, col_list]\n",
    "\n",
    "    # drop any duplicate rows, group by exam, then apply the agg func and output a [exam > bside] mapping dict\n",
    "    bside_agg_dict: dict[float, str] = (\n",
    "        df_subset.drop_duplicates()\n",
    "        .groupby(\"acc_anon\")\n",
    "        .progress_apply(aggregate_bsides)  # type: ignore\n",
    "        .to_dict()\n",
    "    )\n",
    "    return bside_agg_dict\n",
    "\n",
    "\n",
    "# apply the agg function and get a dict of exam mappings\n",
    "scr_bside_agg_dict: dict[float, str] = get_bside_aggregation_dict(mag_scr_add)\n",
    "diag_bside_agg_dict: dict[float, str] = get_bside_aggregation_dict(mag_diag_add)\n",
    "\n",
    "# map the agg dict back to the dataframe\n",
    "mag_scr_add[\"exam_bside\"] = mag_scr_add[\"acc_anon\"].map(scr_bside_agg_dict)\n",
    "display(mag_scr_add[\"exam_bside\"].value_counts(dropna=False))\n",
    "\n",
    "mag_diag_add[\"exam_bside\"] = mag_diag_add[\"acc_anon\"].map(diag_bside_agg_dict)\n",
    "display(mag_diag_add[\"exam_bside\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e1cd9-b7ff-4f4b-8a55-e220314c7022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70fbd604-b80e-4e27-8a80-fab94c5c5c80",
   "metadata": {},
   "source": [
    "## 0.7 Assign Screen BIRADS Helper Variables\n",
    "These are a shorthand for exams with a screening `exam_birads` of 'A' (abnormal) or 'N'/'B' (negative/benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0fe482",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_br_0_list = (\n",
    "    mag_scr_add[mag_scr_add.exam_birads.isin([\"A\"])].acc_anon.unique().tolist()\n",
    ")\n",
    "\n",
    "scr_br_12_list = (\n",
    "    mag_scr_add[mag_scr_add.exam_birads.isin([\"N\", \"B\"])].acc_anon.unique().tolist()\n",
    ")\n",
    "\n",
    "mag_scr_add[\"scr_br_0\"] = False\n",
    "mag_scr_add.loc[mag_scr_add.acc_anon.isin(scr_br_0_list), \"scr_br_0\"] = True\n",
    "\n",
    "mag_scr_add[\"scr_br_12\"] = False\n",
    "mag_scr_add.loc[mag_scr_add.acc_anon.isin(scr_br_12_list), \"scr_br_12\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19119d39-b4f4-478b-a560-88df2e933034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8326978c-e009-42cf-b573-49c8a01b7627",
   "metadata": {},
   "source": [
    "## 0.8 Prepare Target Sample Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85285a6b-e4fb-4a62-bb37-c223436d38ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_score_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # correct variable dtypes and rename columns\n",
    "    rename_dict: dict[str, str] = {\"accession number\": \"acc_anon\", \"Cohort\": \"cohort\"}\n",
    "\n",
    "    data.rename(columns=rename_dict, inplace=True)\n",
    "    return data[[\"acc_anon\", \"cohort\", \"score\"]]\n",
    "\n",
    "\n",
    "# load our score dataframe and prepare it\n",
    "score_df: pd.DataFrame = pd.read_csv(SCORE_PATH)\n",
    "score_df = prepare_score_data(score_df)\n",
    "\n",
    "# ensure key columns have the correct data types\n",
    "score_df[\"acc_anon\"] = pd.to_numeric(score_df[\"acc_anon\"])\n",
    "\n",
    "print(len(score_df))\n",
    "\n",
    "# exclude results from exams with an invalid accession (coded as 999)\n",
    "score_df = score_df[score_df.acc_anon != 999]\n",
    "\n",
    "print(len(score_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaed41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of accessions used in the study then get the subset of magview corresponding to it\n",
    "lunit_acc_list = score_df.acc_anon.tolist()\n",
    "mag_sample = mag_scr_add[mag_scr_add.acc_anon.isin(lunit_acc_list)]\n",
    "\n",
    "# summarize dataframe contents\n",
    "mag_sample.embed.summarize(\"Study Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a28796-a1f5-4ef7-ba8e-22046e668591",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_dict = (\n",
    "    mag_df.drop_duplicates(\"acc_anon\")\n",
    "    .set_index(\"acc_anon\")\n",
    "    .desc.str.contains(\"screen\", case=False)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "score_df.acc_anon.map(scr_dict).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6f4063-1d23-436f-a44c-f6a237d731bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Handle Follow-Up Exam Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ba365",
   "metadata": {},
   "source": [
    "## 1.1 Define Diagnostic/Ultrasound Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fca3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "followup_cols = [\n",
    "    \"acc_anon\",\n",
    "    \"empi_anon\",\n",
    "    \"study_date_anon\",\n",
    "    \"exam_laterality\",\n",
    "    \"exam_birads\",\n",
    "    \"exam_path_severity\",\n",
    "    \"exam_bside\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95adc2da-a221-4355-90ea-f8f798fdef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the relevant columns\n",
    "mag_diag_merge = mag_diag_add.loc[\n",
    "    mag_diag_add.desc.str.contains(\"diag\", case=False), followup_cols\n",
    "].drop_duplicates()\n",
    "\n",
    "# ensure we have exactly 1 row for each exam\n",
    "print(\n",
    "    \"any duplicate exam rows?\", mag_diag_merge.acc_anon.nunique() != len(mag_diag_merge)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19795563-56d2-44e0-b2b1-1ab7230dc4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of magview corresponding to ultrasound exams\n",
    "mag_us = mag_diag_add.loc[mag_diag_add.desc.str.contains(\"US\")]\n",
    "mag_us = mag_us.dropna(subset=\"asses\")\n",
    "mag_us = mag_us[followup_cols].drop_duplicates()\n",
    "\n",
    "# ensure we have exactly 1 row for each exam\n",
    "print(\"any duplicate exam rows?\", mag_us.acc_anon.nunique() != len(mag_us))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5cd334-e496-4dbc-97f7-e8912f4532e3",
   "metadata": {},
   "source": [
    "## 1.2 Get Follow-Up Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3e698-16e4-41dc-9f8c-676305494153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_followup_map_dict(\n",
    "    df: pd.DataFrame, followup_df: pd.DataFrame, time_delta: Union[int, float] = 180\n",
    "):\n",
    "    # don't consider followups with an undefined exam_birads (indicates an invalid birads for that stage)\n",
    "    # followup_df = followup_df[(followup_df.exam_birads != '') & ~pd.isna(followup_df.exam_birads)]\n",
    "\n",
    "    # time delta in days\n",
    "    # expects df to have been corrected for contralateral findings (and for no NA finding sides to exist)\n",
    "    # previous versions assumed no 'B' findings but this does not\n",
    "    merge_df = df.merge(\n",
    "        followup_df, on=\"empi_anon\", how=\"inner\", suffixes=(None, \"_fu\")\n",
    "    )\n",
    "\n",
    "    # ensure exam laterality match, L==L, R==R, or either original/followup is bilateral\n",
    "    merge_df = merge_df.loc[\n",
    "        (merge_df.exam_laterality == merge_df.exam_laterality_fu)\n",
    "        | (merge_df.exam_laterality == \"B\")\n",
    "        | (merge_df.exam_laterality_fu == \"B\")\n",
    "    ]\n",
    "\n",
    "    # exclude followups with an invalid time delta\n",
    "    merge_df[\"fu_delta\"] = (\n",
    "        merge_df.study_date_anon_fu - merge_df.study_date_anon\n",
    "    ).dt.days\n",
    "    merge_df = merge_df.loc[\n",
    "        (merge_df.fu_delta >= 0) & (merge_df.fu_delta <= time_delta)\n",
    "    ]\n",
    "\n",
    "    # get the accession of the first valid followup for each exam and output a dict of mappings\n",
    "    map_dict = (\n",
    "        merge_df.sort_values(\"fu_delta\")\n",
    "        .drop_duplicates(\"acc_anon\", keep=\"first\")\n",
    "        .set_index(\"acc_anon\")[\"acc_anon_fu\"]\n",
    "        .to_dict()\n",
    "    )\n",
    "    return map_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7929b-e725-4d3c-b301-1bfc47dcad79",
   "metadata": {},
   "source": [
    "### 1.2.1 BIRADS 0 Exams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b1c169-74f5-471e-ae13-1d44c7ee5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mag_br0 = mag_contra_df[mag_contra_df.scr_br_0 == True]\n",
    "mag_br0 = mag_sample[mag_sample.scr_br_0 == True]\n",
    "\n",
    "# get birads 0 diagnostic followup map dict\n",
    "br0_dx_map_dict = get_followup_map_dict(mag_br0, mag_diag_merge, time_delta=180)\n",
    "print(f\"{len(br0_dx_map_dict)} valid DX followups found for Screen BIRADS 0s\")\n",
    "\n",
    "# get birads 0 ultrasound followup map dict\n",
    "br0_us_map_dict = get_followup_map_dict(mag_br0, mag_us, time_delta=180)\n",
    "print(f\"{len(br0_us_map_dict)} valid US followups found for Screen BIRADS 0s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c60f9e-014b-4f02-b016-2c6ba108cb83",
   "metadata": {},
   "source": [
    "### 1.2.2 BIRADS 1/2 Exams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd19e9e-6c32-4f91-94bd-b3a43450fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mag_br12 = mag_contra_df[mag_contra_df.scr_br_12 == True]\n",
    "mag_br12 = mag_sample[mag_sample.scr_br_12 == True]\n",
    "\n",
    "# get birads 0 diagnostic followup map dict\n",
    "br12_dx_map_dict = get_followup_map_dict(mag_br12, mag_diag_merge, time_delta=365)\n",
    "print(f\"{len(br12_dx_map_dict)} valid DX followups found for Screen BIRADS 1/2s\")\n",
    "\n",
    "# get birads 0 ultrasound followup map dict\n",
    "br12_us_map_dict = get_followup_map_dict(mag_br12, mag_us, time_delta=365)\n",
    "print(f\"{len(br12_us_map_dict)} valid US followups found for Screen BIRADS 1/2s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d86a6d-0b0a-4be4-9eaa-4ec208816f92",
   "metadata": {},
   "source": [
    "## 1.3 Perform Follow-Up Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc6789",
   "metadata": {},
   "source": [
    "### 1.3.1 Diagnostic Follow-Ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646868d9-abbc-4199-8b72-4b3d6ac0e913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine both dx map dicts and use it to derive a 'earliest_dx_acc' column in mag_sample\n",
    "# which contains any valid followup dx acc and exam-level dx birads/path severity\n",
    "mag_sample[\"earliest_dx_acc\"] = mag_sample[\"acc_anon\"].map(\n",
    "    {**br0_dx_map_dict, **br12_dx_map_dict}\n",
    ")\n",
    "\n",
    "mag_sample = mag_sample.merge(\n",
    "    mag_diag_merge[[\"acc_anon\", \"exam_birads\", \"exam_path_severity\", \"exam_bside\"]],\n",
    "    how=\"left\",\n",
    "    left_on=[\"earliest_dx_acc\"],\n",
    "    right_on=[\"acc_anon\"],\n",
    "    suffixes=(None, \"_dx\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47a9e41-bd1e-4c85-ae92-98497904aa49",
   "metadata": {},
   "source": [
    "### 1.3.2 Ultrasound Follow-Ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7666784-8856-4521-b013-cd48563cbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine both dx map dicts and use it to derive a 'earliest_dx_acc' column in mag_sample\n",
    "# which contains any valid followup dx acc and exam-level dx birads/path severity\n",
    "mag_sample[\"earliest_us_acc\"] = mag_sample[\"acc_anon\"].map(\n",
    "    {**br0_us_map_dict, **br12_us_map_dict}\n",
    ")\n",
    "\n",
    "mag_sample = mag_sample.merge(\n",
    "    mag_us[\n",
    "        [\"acc_anon\", \"exam_birads\", \"exam_path_severity\", \"exam_bside\"]\n",
    "    ].drop_duplicates(),\n",
    "    how=\"left\",\n",
    "    left_on=[\"earliest_us_acc\"],\n",
    "    right_on=[\"acc_anon\"],\n",
    "    suffixes=(None, \"_us\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77c086-2ced-48b3-97dc-9677640f8f33",
   "metadata": {},
   "source": [
    "## 1.4 Summarize Follow-Ups\n",
    "\n",
    "Prioritize results on diagnostic follow-ups if they exist (and are more severe than existing valid ultrasound follow-ups). Otherwise, use any valid ultrasound follow-up results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfdbc4b-dcd6-4360-811e-00836df687ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_sample[\"followup_type\"] = \"\"\n",
    "\n",
    "# present dx (prioritized if both dx and us present)\n",
    "mag_sample.loc[~pd.isna(mag_sample.earliest_dx_acc), \"followup_type\"] = \"DX\"\n",
    "\n",
    "# missing dx, present us\n",
    "mag_sample.loc[\n",
    "    pd.isna(mag_sample.earliest_dx_acc) & ~pd.isna(mag_sample.earliest_us_acc),\n",
    "    \"followup_type\",\n",
    "] = \"US\"\n",
    "\n",
    "# present dx + us, us path_severity more severe so it overrides dx\n",
    "mag_sample.loc[\n",
    "    ~pd.isna(mag_sample.earliest_dx_acc)\n",
    "    & ~pd.isna(mag_sample.earliest_us_acc)\n",
    "    & (mag_sample.exam_path_severity_us < mag_sample.exam_path_severity_dx),\n",
    "    \"followup_type\",\n",
    "] = \"US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99e1c9-d9cc-4e45-9c68-c7267eb7d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_sample[\"followup_path_severity\"] = np.nan\n",
    "mag_sample.loc[mag_sample.followup_type == \"DX\", \"followup_path_severity\"] = (\n",
    "    mag_sample.loc[mag_sample.followup_type == \"DX\", \"exam_path_severity_dx\"]\n",
    ")\n",
    "mag_sample.loc[mag_sample.followup_type == \"US\", \"followup_path_severity\"] = (\n",
    "    mag_sample.loc[mag_sample.followup_type == \"US\", \"exam_path_severity_us\"]\n",
    ")\n",
    "\n",
    "mag_sample[\"followup_bside\"] = \"\"\n",
    "mag_sample.loc[mag_sample.followup_type == \"DX\", \"followup_bside\"] = mag_sample.loc[\n",
    "    mag_sample.followup_type == \"DX\", \"exam_bside_dx\"\n",
    "]\n",
    "mag_sample.loc[mag_sample.followup_type == \"US\", \"followup_bside\"] = mag_sample.loc[\n",
    "    mag_sample.followup_type == \"US\", \"exam_bside_us\"\n",
    "]\n",
    "\n",
    "mag_sample[\"followup_birads\"] = \"\"\n",
    "mag_sample.loc[mag_sample.followup_type == \"DX\", \"followup_birads\"] = mag_sample.loc[\n",
    "    mag_sample.followup_type == \"DX\", \"exam_birads_dx\"\n",
    "]\n",
    "mag_sample.loc[mag_sample.followup_type == \"US\", \"followup_birads\"] = mag_sample.loc[\n",
    "    mag_sample.followup_type == \"US\", \"exam_birads_us\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2fc776-b2db-4c97-b6ff-23bef8bb305c",
   "metadata": {},
   "source": [
    "## 1.5 Assign Follow-Up Helper Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb1f93",
   "metadata": {},
   "source": [
    "### 1.5.1 BIRADS 1/2/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e0617-990e-44bf-9912-5d0b6bcf2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_sample[\"dx_br_123\"] = \"\"\n",
    "mag_sample.loc[(mag_sample.scr_br_0 == True), \"dx_br_123\"] = False\n",
    "mag_sample.loc[\n",
    "    (mag_sample.scr_br_0 == True) & (mag_sample.followup_birads.isin([\"N\", \"B\", \"P\"])),\n",
    "    \"dx_br_123\",\n",
    "] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907f65d-11ae-4c93-933c-607ddb5d2771",
   "metadata": {},
   "source": [
    "### 1.5.2 Pathology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179261ae-e168-4035-b9be-45a1f9661c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name, path_levels in zip(\n",
    "    [\"ps_01\", \"ps_234\", \"ps_5\"], [[0.0, 1.0], [2.0, 3.0, 4.0], [5.0]]\n",
    "):\n",
    "    mag_sample[col_name] = \"\"\n",
    "    mag_sample.loc[(mag_sample.scr_br_0 == True), col_name] = False\n",
    "    mag_sample.loc[\n",
    "        (mag_sample.scr_br_0 == True)\n",
    "        & (\n",
    "            mag_sample.exam_path_severity.isin(path_levels)\n",
    "            | mag_sample.followup_path_severity.isin(path_levels)\n",
    "        ),\n",
    "        col_name,\n",
    "    ] = True\n",
    "\n",
    "mag_sample.loc[mag_sample.ps_01 == True, [\"ps_234\", \"ps_5\"]] = False\n",
    "mag_sample.loc[mag_sample.ps_234 == True, [\"ps_5\"]] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b70ac-9e24-4e61-ae55-7d13f76119aa",
   "metadata": {},
   "source": [
    "## 1.6 Identify Interval Cancers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6988959c-48a3-4e43-b4f8-6dacca7fc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider scr br1/2 exams.\n",
    "# allow bilateral exams with dx path severity in [0.0, 1.0]\n",
    "# allow unilateral exams with dx path severity in [0.0, 1.0] IF the exam laterality and biopsy side match\n",
    "bilat_interval_mask = (\n",
    "    (mag_sample.scr_br_12 == True)\n",
    "    & (mag_sample.exam_laterality == \"B\")\n",
    "    & mag_sample.followup_path_severity.isin([0.0, 1.0])\n",
    ")\n",
    "unilat_interval_mask = (\n",
    "    (mag_sample.scr_br_12 == True)\n",
    "    & (mag_sample.exam_laterality != \"B\")\n",
    "    & mag_sample.followup_path_severity.isin([0.0, 1.0])\n",
    "    & (mag_sample.exam_laterality == mag_sample.followup_bside)\n",
    ")\n",
    "\n",
    "mag_sample[\"interval_cancer\"] = \"\"\n",
    "mag_sample.loc[(mag_sample.scr_br_12 == True), \"interval_cancer\"] = False\n",
    "mag_sample.loc[bilat_interval_mask | unilat_interval_mask, \"interval_cancer\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b9f49-4393-4096-8e31-92647f9b568a",
   "metadata": {},
   "source": [
    "## 1.7 Evaluate Long-Term Follow-Up Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca10d1c-d055-4c5e-a8b8-b4ca0732ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_condition(df, acc_list, span_list, date_col: str = \"study_date_anon\"):\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df[\"empi_anon\"] = pd.to_numeric(df[\"empi_anon\"])\n",
    "    df[\"acc_anon\"] = pd.to_numeric(df[\"acc_anon\"])\n",
    "    condition_dict = dict()\n",
    "\n",
    "    # # get a list of all qualifying conditions to do the first pass elim\n",
    "    # qual_list = [span['qualifying'] for span in span_list if span['qualifying'] != \"\"]\n",
    "\n",
    "    # # concatenate all qual strings into an or condition for an initial pass\n",
    "    # init_qual_str = \" | \".join([f\"({qual_str})\" for qual_str in qual_list])\n",
    "    df_acc_list = df.acc_anon.unique().tolist()\n",
    "    acc_list = list(set(acc_list).intersection(set(df_acc_list)))\n",
    "\n",
    "    # iterate over exams\n",
    "    for target_acc in tqdm(acc_list):\n",
    "        # init bool to track whether the acc has been accepted/rejected\n",
    "        acc_is_valid = True\n",
    "\n",
    "        # is there a faster way to do this?\n",
    "        acc_mask = df.acc_anon == target_acc\n",
    "        date_i = df.loc[acc_mask, date_col].mode()[0]\n",
    "        empi = df.loc[acc_mask, \"empi_anon\"].mode()[0]\n",
    "\n",
    "        patient_df = df[df.empi_anon == empi]\n",
    "\n",
    "        # iterate over spans\n",
    "        for span in span_list:\n",
    "            # determine span parameters\n",
    "            span_length = span[\"length\"]  # throw an error if no span length was given\n",
    "            span_qual = span.get(\"qualifying\", \"\")\n",
    "            span_disqual = span.get(\"disqualifying\", \"\")\n",
    "            span_n = span.get(\n",
    "                \"required_n\", 0 if not span_qual else 1\n",
    "            )  # default to 1 if there's a qualifying condition\n",
    "\n",
    "            # get span end date\n",
    "            date_f = date_i + pd.Timedelta(days=span_length)\n",
    "\n",
    "            # find subset of patient df between the span start/end dates\n",
    "            span_df = patient_df[\n",
    "                (patient_df[date_col] >= date_i) & (patient_df[date_col] <= date_f)\n",
    "            ]\n",
    "\n",
    "            # evaluate qual condition if it exists, else keep span_df\n",
    "            span_df = span_df.query(span_qual) if span_qual else span_df\n",
    "\n",
    "            # if span_df < n cases, reject the acc\n",
    "            if span_df.acc_anon.nunique() < span_n:\n",
    "                acc_is_valid = False\n",
    "                break\n",
    "\n",
    "            # otherwise if we have a qual condition w/ sufficient exams, + a disqual condition to eval\n",
    "            elif span_qual and span_disqual:\n",
    "                # get the study date of the nearest valid exam, and filter span_df to exclude cases later than it\n",
    "                # so we only check the disqual condition up to this point\n",
    "                new_date_f = span_df[date_col].sort_values().unique().tolist()[0]\n",
    "\n",
    "                # reformat the span_disqual string to include the new cutoff date condition\n",
    "                span_disqual += f\" & ({date_col} <= '{new_date_f}')\"\n",
    "\n",
    "            if span_disqual:\n",
    "                # if there are any disqualifying cases, reject the acc\n",
    "                if span_df.query(span_disqual).acc_anon.nunique():\n",
    "                    acc_is_valid = False\n",
    "                    break\n",
    "\n",
    "            # increment date_0 before evaluating the next span\n",
    "            # we only need to do this if the last span passes\n",
    "            date_i = date_f + pd.Timedelta(days=1)\n",
    "\n",
    "        # if all spans pass, accept the acc\n",
    "        if acc_is_valid:\n",
    "            condition_dict[target_acc] = True\n",
    "        else:\n",
    "            condition_dict[target_acc] = False\n",
    "\n",
    "    # return the condition dict after evaluating all exams\n",
    "    return condition_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e55ed-20e8-43b7-8faf-529074a17fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only evaluate followup status for negative exams\n",
    "scr_neg_acc_list = (\n",
    "    mag_sample.loc[\n",
    "        (mag_sample.scr_br_12 == True) & (mag_sample.interval_cancer == False),\n",
    "        \"acc_anon\",\n",
    "    ]\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "print(\"screen negatives accs:\", len(scr_neg_acc_list))\n",
    "\n",
    "neg_acc_list = (\n",
    "    mag_sample.loc[\n",
    "        ~((mag_sample.scr_br_0 == True) & (mag_sample.ps_01 == True)), \"acc_anon\"\n",
    "    ]\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "print(\"all negatives accs:\", len(neg_acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9aa318-96d1-49d1-aaa1-5343c1d47234",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_contra_merge = pd.concat([mag_scr_add, mag_diag_add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3628ade-8fb2-49e2-89b9-540c7ada8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_list = [\n",
    "    {  # the first span has no conditions, so any exams here are ignored\n",
    "        \"length\": 365,\n",
    "        \"qualifying\": \"\",\n",
    "        \"disqualifying\": \"\",\n",
    "        \"required_n\": 0,\n",
    "    },\n",
    "    {  # the second span accepts accs with >=1 of *any* exam type\n",
    "        \"length\": 365 * 3,\n",
    "        \"qualifying\": \"\",\n",
    "        \"disqualifying\": \"\",\n",
    "        \"required_n\": 1,  # required_n applies to the qualifying condition, must be >1 exams of any kind during this period\n",
    "    },\n",
    "]\n",
    "\n",
    "any_followup_dict = verify_condition(mag_contra_merge, scr_neg_acc_list, span_list)\n",
    "\n",
    "mag_sample[\"any_followup\"] = mag_sample.acc_anon.map(any_followup_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c033809-9ba9-499a-930e-71f842ae4e0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Data Finalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de44aa5-6b5f-473b-9c59-4c953b95b41a",
   "metadata": {},
   "source": [
    "## 2.1 Data Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b66198-24d1-483c-adb3-3efcdc62e6f1",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1.1 Major Exam Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495fcf68-fb8d-4384-a8e0-e24e67fd09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_sample[\"label\"] = \"INVALID\"\n",
    "mag_sample[\"label\"] = mag_sample[\"label\"].case_when([\n",
    "    (mag_sample.eval(\"(scr_br_12 == True) & (interval_cancer == False)\"), \"Screen Negative\"), \n",
    "    (mag_sample.eval(\"(scr_br_12 == True) & (interval_cancer == True)\"), \"Interval Cancer\"),\n",
    "    (mag_sample.eval(\"(scr_br_0 == True) & (ps_01 == True)\"), \"Screen Detected Cancer\"),\n",
    "    (mag_sample.eval(\"(scr_br_0 == True) & (ps_234 == True)\"), \"Biopsy Proven Benign\"),\n",
    "    (mag_sample.eval(\"(scr_br_0 == True) & (dx_br_123 == True)\"), \"Diagnostic Negative\"),\n",
    "    (mag_sample.eval(\"(scr_br_0 == True) & (ps_5 == True)\"), \"Other Cancer\"),\n",
    "])  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306391d9-35a3-47c7-a74c-ee6463a63bb0",
   "metadata": {},
   "source": [
    "### 2.1.2 Finding Characteristics\n",
    "First, we'll derive characteristics for all findings in the sample. Then we'll aggregate these upwards so exam-level characteristics only consider abnormal findings in abnormal exams, and negative/benign findings in negative/benign exams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18acb3c9-a799-4a33-93ce-2e5c60eabdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_sample.calcdistri = mag_sample.calcdistri.fillna(\"\")\n",
    "mag_sample.calcdistri = mag_sample.calcdistri.str.strip().astype(str)\n",
    "mag_sample.calcdistri.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b535b-7135-4b9f-8f7e-a91f3c600e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_sample.calcfind = mag_sample.calcfind.fillna(\"\")\n",
    "mag_sample.calcfind = mag_sample.calcfind.str.strip().astype(str)\n",
    "mag_sample.calcfind.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7840d-2cab-4208-a41c-c46d8855ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_sample.calcnumber = mag_sample.calcnumber.fillna(0.0)\n",
    "mag_sample.calcnumber = mag_sample.calcnumber.astype(float)\n",
    "mag_sample.calcnumber.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e08414-c6fa-4ae2-8fba-83ce707d74c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive findings-level characteristics\n",
    "mag_sample[[\"mass\", \"asymmetry\", \"arch_distortion\", \"calcification\"]] = mag_sample.progress_apply(\n",
    "    embed_params.extract_characteristics, axis=\"columns\", result_type=\"expand\"\n",
    ")  # type: ignore\n",
    "\n",
    "# finding characteristics should only be present on benign/abnormal findings\n",
    "mag_sample.loc[\n",
    "    mag_sample.asses == \"N\", [\"mass\", \"asymmetry\", \"arch_distortion\", \"calcification\"]\n",
    "] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c250c7-e8a9-4176-a743-f0f1cb36a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all exams with a birads A finding and filter them so they only have birads A findings\n",
    "br0_exams = mag_sample[\n",
    "    mag_sample.exam_birads.isin([\"A\"]) & mag_sample.asses.isin([\"A\"])\n",
    "]\n",
    "br12_exams = mag_sample[mag_sample.exam_birads.isin([\"N\", \"B\"])]\n",
    "\n",
    "# generalize to exam-level\n",
    "br0_exam_chars_dict = br0_exams.groupby(\"acc_anon\").progress_apply(embed_params.aggregate_characteristics).to_dict()  # type: ignore\n",
    "br12_exam_chars_dict = br12_exams.groupby(\"acc_anon\").progress_apply(embed_params.aggregate_characteristics).to_dict()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "for char_var in [\"exam_mass\", \"exam_asymmetry\", \"exam_arch_distortion\", \"exam_calcification\"]:\n",
    "    br12_char_dict = {k: v[char_var] for k, v in br12_exam_chars_dict.items()}\n",
    "    br0_char_dict = {k: v[char_var] for k, v in br0_exam_chars_dict.items()}\n",
    "\n",
    "    mag_sample[char_var] = mag_sample.acc_anon.map({**br12_char_dict, **br0_char_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b5681-fc94-4f1f-9251-7ac1f3deec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the only exams with any missing values have invalid BIRADS and will\n",
    "# be dropped, so they can be considered false\n",
    "mag_sample = mag_sample.fillna(\n",
    "    {\n",
    "        \"exam_mass\": False,\n",
    "        \"exam_asymmetry\": False,\n",
    "        \"exam_arch_distortion\": False,\n",
    "        \"exam_calcification\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d1828-8d3c-4587-a670-e4f6290763be",
   "metadata": {},
   "source": [
    "### 2.1.3 Screen Detected Pathology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ea8ba-a3bf-4c47-b47e-332ad4ba1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_sample[\"scr_detected_path\"] = \"No Pathology\"\n",
    "mag_sample.loc[mag_sample.scr_br_0 == True, \"scr_detected_path\"] = mag_sample.loc[mag_sample.scr_br_0 == True, \"scr_detected_path\"].case_when([\n",
    "    (mag_sample.eval(\"(exam_path_severity == 0.0) | ((exam_path_severity.isna() | (followup_path_severity < exam_path_severity)) & (followup_path_severity == 0.0))\"), \"Invasive Cancer\"), \n",
    "    (mag_sample.eval(\"(exam_path_severity == 1.0) | ((exam_path_severity.isna() | (followup_path_severity < exam_path_severity)) & (followup_path_severity == 1.0))\"), \"Noninvasive Cancer\"),\n",
    "    (mag_sample.eval(\"(exam_path_severity == 2.0) | ((exam_path_severity.isna() | (followup_path_severity < exam_path_severity)) & (followup_path_severity == 2.0))\"), \"High Risk Lesion\"),\n",
    "    (mag_sample.eval(\"(exam_path_severity == 3.0) | ((exam_path_severity.isna() | (followup_path_severity < exam_path_severity)) & (followup_path_severity == 3.0))\"), \"Borderline Lesion\"),\n",
    "    (mag_sample.eval(\"(exam_path_severity == 4.0) | ((exam_path_severity.isna() | (followup_path_severity < exam_path_severity)) & (followup_path_severity == 4.0))\"), \"Benign Lesion\"),\n",
    "    (mag_sample.eval(\"(exam_path_severity == 5.0) | ((exam_path_severity.isna() | (followup_path_severity < exam_path_severity)) & (followup_path_severity == 5.0))\"), \"Other Cancer\"),\n",
    "]) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cfddeb-b652-400d-aed9-2afbfe3dea55",
   "metadata": {},
   "source": [
    "## 2.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0421345e-6ec8-4395-bece-9e6b2e0fe2e9",
   "metadata": {},
   "source": [
    "### 2.2.1 Patient Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf10654-ec8c-4018-80a8-50b3cae0de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize whitespace\n",
    "mag_sample['ETHNICITY_DESC'] = mag_sample['ETHNICITY_DESC'].str.strip()\n",
    "mag_sample['ETHNICITY_DESC'] = mag_sample['ETHNICITY_DESC'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "mag_sample[\"race\"] = \"Other\"\n",
    "mag_sample[\"race\"] = mag_sample[\"race\"].case_when([\n",
    "    (mag_sample.eval(\"ETHNICITY_DESC == 'African American or Black'\"), \"Black\"),\n",
    "    (mag_sample.eval(\"ETHNICITY_DESC == 'Caucasian or White'\"), \"White\"),\n",
    "    (mag_sample.eval(\"ETHNICITY_DESC == 'Asian'\"), \"Asian\"),\n",
    "    (mag_sample.eval(\"ETHNICITY_DESC == 'Unknown, Unavailable or Unreported'\"), \"Unknown\"),\n",
    "    (mag_sample.eval(\"ETHNICITY_DESC.isna()\"), \"Unknown\")\n",
    "]) # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef6485-ac12-4222-8b47-4f50e2e68256",
   "metadata": {},
   "source": [
    "### 2.2.2 Patient Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18857651-32c1-4279-b249-342e4662d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize whitespace\n",
    "mag_sample[\"ETHNIC_GROUP_DESC\"] = mag_sample[\"ETHNIC_GROUP_DESC\"].str.strip()\n",
    "mag_sample[\"ETHNIC_GROUP_DESC\"] = mag_sample[\"ETHNIC_GROUP_DESC\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "hispanic_list = [\"Hispanic or Latino\", \"Unknown~Hispanic\"]\n",
    "not_hispanic_list = [\"Non-Hispanic or Latino\", \"Non-Hispanic~Unknown\", \"Unknown~Non-Hispanic\"]\n",
    "\n",
    "mag_sample[\"ethnicity\"] = \"Unknown\"\n",
    "mag_sample[\"ethnicity\"] = mag_sample[\"ethnicity\"].case_when([\n",
    "    (mag_sample.eval(\"ETHNIC_GROUP_DESC.isin(@hispanic_list)\"), \"Hispanic or Latino\"),\n",
    "    (mag_sample.eval(\"ETHNIC_GROUP_DESC.isin(@not_hispanic_list)\"), \"Not Hispanic or Latino\"),\n",
    "]) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839498fa-23cd-4a82-88bb-f24ec983afa2",
   "metadata": {},
   "source": [
    "### 2.2.3 Patient Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69347e45-d6b4-4888-82ed-5e3c5b5014c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_sample[\"age_binned\"] = pd.cut(\n",
    "    mag_sample[\"age_at_study\"], bins=[0, 50, 75, 120], labels=[\"<50\", \"50-75\", \">=75\"]\n",
    ")\n",
    "mag_sample.drop_duplicates(\"acc_anon\")[\"age_binned\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6056d4-2a0d-4b41-8b41-43752657c3f9",
   "metadata": {},
   "source": [
    "## 2.3 Finalize and Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041d726-d851-4ae9-9cbe-ade4fe2144f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure tissueden is numeric\n",
    "mag_sample[\"tissueden\"] = pd.to_numeric(mag_sample[\"tissueden\"], errors=\"coerce\")\n",
    "\n",
    "# get the accessions of any exams with invalid screening-stage BIRADS\n",
    "dx_only_asses = [\"P\", \"S\", \"M\", \"K\"]\n",
    "invalid_exam_list = (\n",
    "    mag_sample.loc[\n",
    "        mag_sample.desc.str.contains(\"screen\", case=False)\n",
    "        & mag_sample.asses.isin(dx_only_asses),\n",
    "        \"acc_anon\",\n",
    "    ]\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6e9f8-c294-49a1-974b-77843cc63437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any cases where tissueden is 5.0, with no valid major label, or with other cancers present\n",
    "final_df = mag_sample[\n",
    "    ~(\n",
    "        (mag_sample.tissueden == 5.0)\n",
    "        | (mag_sample.acc_anon.isin(invalid_exam_list))\n",
    "        | (mag_sample.label == \"INVALID\")\n",
    "        | (mag_sample.label == \"Other Cancer\")\n",
    "        | (mag_sample.scr_detected_path == \"Other Cancer\")\n",
    "        | (mag_sample.any_followup == False)\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6563ac0-e063-4949-9926-fbc8b19d2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH: str = \"../../data/6.2_labeled_mag_sample.csv\"\n",
    "\n",
    "final_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"data saved to: '{OUTPUT_PATH}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3a97f9-e3b6-4287-903b-6bd76b5f1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78b2d3-1e30-46d5-9a05-fe1e1efd8cef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3.0 Output Exam/Finding Level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b54f1-d297-43e2-a708-540d03c2191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FILES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d25bd-156d-41e1-832e-229a079d11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format dtypes\n",
    "final_df[\"empi_anon\"] = pd.to_numeric(final_df[\"empi_anon\"])\n",
    "final_df[\"acc_anon\"] = pd.to_numeric(final_df[\"acc_anon\"])\n",
    "final_df[\"exam_birads\"] = final_df[\"exam_birads\"].astype(str)\n",
    "final_df[\"asses\"] = final_df[\"asses\"].astype(str)\n",
    "score_df[\"acc_anon\"] = pd.to_numeric(score_df[\"acc_anon\"])\n",
    "\n",
    "# merge exam scores onto the sample data\n",
    "output_df = final_df.merge(score_df, how=\"inner\", on=\"acc_anon\")\n",
    "output_df.embed.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f1bbd-8755-46c2-9a32-b82e3a4646a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1887a0fa-0836-46f9-91bd-82360e3691b3",
   "metadata": {},
   "source": [
    "## 3.1 Prepare Finding-Level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e37ed9f-f82f-4e76-bc03-55a4f4bd59a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_LEVEL = \"finding\"\n",
    "level_unique_func = embed_params.count_findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc549de1-99e2-4c84-816a-b7aa1c07ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = output_df\n",
    "\n",
    "# validate whether dataframe has duplicates\n",
    "len_df = len(df)\n",
    "level_unique = level_unique_func(df)\n",
    "rows_duplicated = level_unique != len_df\n",
    "\n",
    "print(f\"unique {FILE_LEVEL}s:  {level_unique}\")\n",
    "print(f\"dataframe rows:  {len_df}\")\n",
    "print(f\"duplicate {FILE_LEVEL} rows?:  {rows_duplicated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1558dea8-0c86-4354-a3ce-33612a40f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have some findings that were added by our addendums (but have no registered finding num), since these are so few we know each of these\n",
    "# is not bilateral, so we'll just manually increment their finding number by the exam max + 1\n",
    "for acc, group in output_df[pd.isna(output_df.numfind)].groupby(\"acc_anon\"):\n",
    "    max_numfind = output_df.loc[output_df.acc_anon == acc, \"numfind\"].max()\n",
    "    output_df.loc[group.index[0], \"numfind\"] = max_numfind + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23ab44d-8a85-4d81-81fe-81e4ec71f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "finding_df = output_df.copy()\n",
    "\n",
    "drop_cols = [\"acc_lat\"]\n",
    "finding_df = finding_df.drop(columns=drop_cols)\n",
    "\n",
    "finding_df[\"acc_find\"] = (\n",
    "    finding_df[\"acc_anon\"].astype(str) + \"_\" + finding_df[\"numfind\"].astype(str)\n",
    ")\n",
    "\n",
    "patient_feature_list = embed_params.list_columns([\"patient\"])\n",
    "patient_feature_list = [\n",
    "    s\n",
    "    for s in patient_feature_list\n",
    "    if s not in [\"GENDER_DESC\", \"ETHNICITY_DESC\", \"ETHNIC_GROUP_DESC\", \"cohort_num\"]\n",
    "]\n",
    "\n",
    "exam_feature_list = embed_params.list_columns([\"exam\"])\n",
    "exam_feature_list = [\n",
    "    s\n",
    "    for s in exam_feature_list\n",
    "    if s not in [\"tech_init\", \"init\", \"proccode\", \"case\", \"sdate_anon\"]\n",
    "]\n",
    "\n",
    "findings_feature_list = embed_params.list_columns([\"finding\"])\n",
    "\n",
    "additional_feature_list = [\n",
    "    \"mass\",\n",
    "    \"asymmetry\",\n",
    "    \"arch_distortion\",\n",
    "    \"calcification\",\n",
    "    \"age_binned\",\n",
    "    \"race\",\n",
    "    \"ethnicity\",\n",
    "    \"score\",\n",
    "    \"label\",\n",
    "    \"exam_laterality\",\n",
    "    \"exam_birads\",\n",
    "    \"scr_detected_path\",\n",
    "    \"followup_type\",\n",
    "    \"followup_birads\",\n",
    "    \"followup_path_severity\",\n",
    "    \"exam_mass\",\n",
    "    \"exam_asymmetry\",\n",
    "    \"exam_arch_distortion\",\n",
    "    \"exam_calcification\",\n",
    "]\n",
    "total_feature_list = [\n",
    "    *patient_feature_list,\n",
    "    *exam_feature_list,\n",
    "    *findings_feature_list,\n",
    "    *additional_feature_list,\n",
    "]\n",
    "\n",
    "path_cols = [\"path1\", \"path2\", \"path3\", \"path4\", \"path5\"]\n",
    "find_path_cols = [f\"find_{s}\" for s in path_cols]\n",
    "\n",
    "finding_df = finding_df.drop_duplicates(subset=total_feature_list + path_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80873d3-db42-409e-9248-ba1b443c90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = finding_df\n",
    "\n",
    "# validate whether dataframe has duplicates\n",
    "len_df = len(df)\n",
    "level_unique = level_unique_func(df)\n",
    "rows_duplicated = level_unique != len_df\n",
    "\n",
    "print(f\"unique {FILE_LEVEL}s:  {level_unique}\")\n",
    "print(f\"dataframe rows:  {len_df}\")\n",
    "print(f\"duplicate {FILE_LEVEL} rows?:  {rows_duplicated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe2fa19-4a7d-4d81-b171-373811adea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "finding_df[\"acc_find\"] = (\n",
    "    finding_df[\"acc_anon\"].astype(str) + \"_\" + finding_df[\"numfind\"].astype(str)\n",
    ")\n",
    "\n",
    "acc_find_path_dict = dict()\n",
    "\n",
    "for group_acc_find, group_df in tqdm(\n",
    "    finding_df.groupby(\"acc_find\"), total=level_unique\n",
    "):\n",
    "    if all(pd.isna(group_df.path_severity)):\n",
    "        continue\n",
    "\n",
    "    acc_find_path_list = []\n",
    "    for col in path_cols:\n",
    "        acc_find_path_list.extend(group_df[col].dropna().unique().tolist())\n",
    "\n",
    "    acc_find_path_dict[group_acc_find] = list(set(acc_find_path_list))[:5]\n",
    "\n",
    "finding_df[find_path_cols] = \"\"\n",
    "\n",
    "for acc_find, path_list in tqdm(acc_find_path_dict.items()):\n",
    "    acc_find_mask = finding_df.acc_find == acc_find\n",
    "\n",
    "    for i, path_type in enumerate(path_list):\n",
    "        col = find_path_cols[i]\n",
    "        finding_df.loc[acc_find_mask, col] = path_type\n",
    "\n",
    "finding_df = finding_df.drop_duplicates(subset=total_feature_list + find_path_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ba3b3-ebab-4a9b-81d3-5b1d703bd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "br0_acc_set = set(finding_df[finding_df.exam_birads == \"A\"].acc_anon.unique())\n",
    "br0_find_acc_set = set(\n",
    "    finding_df[\n",
    "        (finding_df.exam_birads == \"A\") & (finding_df.asses == \"A\")\n",
    "    ].acc_anon.unique()\n",
    ")\n",
    "\n",
    "nofind_acc_list = list(br0_acc_set.difference(br0_find_acc_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176a432-1fc6-4e44-92e1-542d0fa9cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop non-BIRADS A findings in exams with any BIRADS A findings\n",
    "finding_df = finding_df[\n",
    "    finding_df.acc_anon.isin(nofind_acc_list)\n",
    "    | ~((finding_df.exam_birads == \"A\") & (finding_df.asses != \"A\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f4025-2331-4729-b03b-c355aa91c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exam_finding_sides(group: pd.DataFrame) -> str:\n",
    "    sides = \"\".join(group.side.tolist())\n",
    "    return sides\n",
    "\n",
    "\n",
    "acc_find_sides_dict = (\n",
    "    finding_df.groupby(\"acc_find\").progress_apply(get_exam_finding_sides).to_dict()  # type: ignore\n",
    ")\n",
    "finding_df[\"acc_find_sides\"] = finding_df.acc_find.map(acc_find_sides_dict)\n",
    "finding_df[\"acc_find_sides\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8c383-c278-463e-933e-1996c0d61adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for acc_find, group in finding_df[finding_df[\"acc_find_sides\"] == \"RR\"].groupby(\"acc_find\"):\n",
    "    display(acc_find)\n",
    "    display(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b554487d-9aa9-4cca-a5cc-12451727bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finding_df.loc[finding_df.acc_find_sides == \"LR\", \"side\"] = \"B\"\n",
    "finding_df = finding_df.drop_duplicates(subset=[\"acc_find\", \"side\", \"path_severity\"])\n",
    "\n",
    "finding_df[\"acc_find_sides\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9613bf-1d5d-4046-98c4-bd1f799eb903",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = finding_df\n",
    "\n",
    "# validate whether dataframe has duplicates\n",
    "len_df = len(df)\n",
    "level_unique = level_unique_func(df)\n",
    "rows_duplicated = level_unique != len_df\n",
    "\n",
    "print(f\"unique {FILE_LEVEL}s:  {level_unique}\")\n",
    "print(f\"dataframe rows:  {len_df}\")\n",
    "print(f\"duplicate {FILE_LEVEL} rows?:  {rows_duplicated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0bb35b-27ba-4aed-a162-6d50af6e3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file if valid+enabled\n",
    "if SAVE_FILES & rows_duplicated:\n",
    "    print(\"File saving enabled, but duplicates detected -- skipping\")\n",
    "elif SAVE_FILES:\n",
    "    save_path = f\"../../data/6.2_{FILE_LEVEL}_level.csv\"\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"File saving enabled, {FILE_LEVEL.title()}-level data saved to: {save_path}\")\n",
    "else:\n",
    "    print(\"File saving disabled -- skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d92cc3-5f46-4e92-8131-fa11ce1b4e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3163d6a-6abc-4d0e-bfa9-16eb24208007",
   "metadata": {},
   "source": [
    "## 3.2 Prepare Exam-Level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb06ca-358f-4b62-8470-22c1627f15b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_LEVEL = \"exam\"\n",
    "level_unique_func = embed_params.count_exams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6053e6-7871-46cd-a3c5-cf63473befe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = output_df\n",
    "\n",
    "# validate whether dataframe has duplicates\n",
    "len_df = len(df)\n",
    "level_unique = level_unique_func(df)\n",
    "rows_duplicated = level_unique != len_df\n",
    "\n",
    "print(f\"unique {FILE_LEVEL}s:  {level_unique}\")\n",
    "print(f\"dataframe rows:  {len_df}\")\n",
    "print(f\"duplicate {FILE_LEVEL} rows?:  {rows_duplicated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4737a58-b1fb-4c5b-a924-7e4753344ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of patient/exam-level features\n",
    "patient_feature_list = embed_params.list_columns([\"patient\"])\n",
    "patient_feature_list = [\n",
    "    s\n",
    "    for s in patient_feature_list\n",
    "    if s not in [\"GENDER_DESC\", \"ETHNICITY_DESC\", \"ETHNIC_GROUP_DESC\", \"cohort_num\"]\n",
    "]\n",
    "\n",
    "exam_feature_list = embed_params.list_columns([\"exam\"])\n",
    "exam_feature_list = [\n",
    "    s\n",
    "    for s in exam_feature_list\n",
    "    if s not in [\"tech_init\", \"init\", \"proccode\", \"case\", \"sdate_anon\"]\n",
    "]\n",
    "\n",
    "additional_feature_list = [\n",
    "    \"age_binned\",\n",
    "    \"race\",\n",
    "    \"ethnicity\",\n",
    "    \"score\",\n",
    "    \"label\",\n",
    "    \"exam_laterality\",\n",
    "    \"exam_birads\",\n",
    "    \"scr_detected_path\",\n",
    "    \"followup_type\",\n",
    "    \"followup_birads\",\n",
    "    \"followup_path_severity\",\n",
    "    \"exam_mass\",\n",
    "    \"exam_asymmetry\",\n",
    "    \"exam_arch_distortion\",\n",
    "    \"exam_calcification\",\n",
    "]\n",
    "total_feature_list = [\n",
    "    *patient_feature_list,\n",
    "    *exam_feature_list,\n",
    "    *additional_feature_list,\n",
    "]\n",
    "\n",
    "output_df.first_3_zip = pd.to_numeric(output_df.first_3_zip, errors=\"coerce\").astype(\n",
    "    float\n",
    ")\n",
    "exam_df = output_df[total_feature_list].drop_duplicates()\n",
    "\n",
    "# drop any remaining duplicates\n",
    "exam_df = exam_df.sort_values(\"exam_birads\").drop_duplicates(\"acc_anon\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dcd84f-634f-49cb-befd-94f24c3eb03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = exam_df\n",
    "\n",
    "# validate whether dataframe has duplicates\n",
    "len_df = len(df)\n",
    "level_unique = level_unique_func(df)\n",
    "rows_duplicated = level_unique != len_df\n",
    "\n",
    "print(f\"unique {FILE_LEVEL}s:  {level_unique}\")\n",
    "print(f\"dataframe rows:  {len_df}\")\n",
    "print(f\"duplicate {FILE_LEVEL} rows?:  {rows_duplicated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fdfe98-713b-4a49-95f2-99d1fcd730bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file if valid+enabled\n",
    "if SAVE_FILES & rows_duplicated:\n",
    "    print(\"File saving enabled, but duplicates detected -- skipping\")\n",
    "elif SAVE_FILES:\n",
    "    save_path = f\"../../data/6.2_{FILE_LEVEL}_level.csv\"\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"File saving enabled, {FILE_LEVEL.title()}-level data saved to: {save_path}\")\n",
    "else:\n",
    "    print(\"File saving disabled -- skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee185425-cd38-49b8-bfba-3d4508ad6be6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@deathbeds/jupyterlab-fonts": {
   "fontLicenses": {},
   "fonts": {},
   "styles": {
    ":root": {}
   }
  },
  "kernelspec": {
   "display_name": "dsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
